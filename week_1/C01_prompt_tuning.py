import torch
import torch.nn as nn
from transformers import AutoModelForCausalLM, AutoTokenizer

# ===============================
# 1. CONFIG
# ===============================
MODEL_NAME = "Qwen/Qwen2.5-Coder-1.5B-Instruct"
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

N_PROMPT_TOKENS = 20
LR = 1e-4   
EPOCHS = 50
MAX_NEW_TOKENS = 64

# ===============================
# 2. TRAINING DATA
# ===============================
train_data = [
    (
        "Move forward 2 meters",
        "ros2 topic pub /cmd_vel geometry_msgs/msg/Twist \"{linear: {x: 2.0}}\""
    ),
    (
        "Turn left 90 degrees",
        "ros2 service call /rotate_robot robot_msgs/srv/Rotate \"{angle: 1.57}\""
    ),
    (
        "Navigate to waypoint A",
        "ros2 action send_goal /navigate_to_pose nav2_msgs/action/NavigateToPose "
        "\"{pose: {header: {frame_id: 'map'}, pose: {position: {x: 5.0, y: 2.0}}}}\""
    )
]

# ===============================
# 3. LOAD MODEL & TOKENIZER
# ===============================
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    torch_dtype=torch.bfloat16 if DEVICE == "cuda" else torch.float32,
    device_map="auto"
)

for p in model.parameters():
    p.requires_grad = False

# Inference/á€…á€™á€ºá€¸á€á€•á€ºá€á€»á€„á€ºá€›á€„á€º 
model.eval() 

# ===============================
# 4. SOFT PROMPT MODULE
# ===============================
class SoftPrompt(nn.Module):
    def __init__(self, n_tokens, embedding_layer):
        super().__init__()

        # vector table á€™á€¾ row 20 á€€á€­á€¯ á€šá€°á€•á€¼á€®á€¸ initialize á€œá€¯á€•á€ºá€á€¼á€„á€ºá€¸ . ([20, 1536])
        init_prompt = embedding_layer.weight[:n_tokens].detach().clone()

        # ([20, 1536])
        self.prompt_embeddings = nn.Parameter(init_prompt) 
        print("shape after initialization ( SoftPrompt::init ):", self.prompt_embeddings.shape)

    def forward(self, batch_size):
        blabla = self.prompt_embeddings.unsqueeze(0).expand(batch_size, -1, -1)
        print("shape after unsqueeze ( SoftPrompt::forward ):", blabla.shape)
        return blabla

# vector table á€›á€šá€°á€á€¼á€„á€ºá€¸ row 151936, col 1536
embedding_layer = model.get_input_embeddings()

# torch.Size([151936, 1536])
print("shape after embedding_layer ( main ):",embedding_layer.weight.shape)

soft_prompt = SoftPrompt(
    N_PROMPT_TOKENS,
    embedding_layer
).to(embedding_layer.weight.device)


# ===============================
# 5. TRAINING FUNCTION (FIXED)
# ===============================
def compute_loss(input_text, target_text):
    # ([1, 6]) 6 = token count of "Move forward 2 meters"
    input_ids = tokenizer(input_text, return_tensors="pt").input_ids.to(model.device)
    target_ids = tokenizer(target_text, return_tensors="pt").input_ids.to(model.device)

    batch_size = input_ids.size(0)
    print("input id shape ( compute_loss ): ",input_ids.shape)


    # Concatenate tokens
    full_ids = torch.cat([input_ids, target_ids], dim=1)
    # input tokens = 6 , target tokens = 15 , full tokens = 21 , [1, 21]
    print("full id shape ( compute_loss ): ",full_ids.shape)

    # Embeddings ,  shape: [1, 21, 1536]
    token_embeds = model.get_input_embeddings()(full_ids)
    print("token embed shape ( compute_loss ): ",token_embeds.shape)

    # Soft Prompt Embeddings , shape: [1, 20, 1536]
    prompt_embeds = soft_prompt(batch_size)
    print("prompt embed shape ( compute_loss ): ",prompt_embeds.shape)

    # Final embeddings: [PROMPT][INPUT][TARGET]
    full_embeds = torch.cat([prompt_embeds, token_embeds], dim=1)
    print("full embed shape ( compute_loss ): ",full_embeds.shape)

    # ğŸ”’ ATTENTION MASK (THIS FIXES NaN)
    # attention_mask = 1 á€†á€­á€¯á€á€¬ "á€’á€® token á€€á€­á€¯ mask á€™á€œá€¯á€•á€ºá€˜á€°á€¸, á€á€á€­á€‘á€¬á€¸á€•á€«" á€œá€­á€¯á€· á€†á€­á€¯á€œá€­á€¯á€•á€«á€á€šá€ºá‹
    attention_mask = torch.ones(
        full_embeds.size()[:-1],
        device=model.device,
        dtype=torch.long
    )
    print("attention mask shape ( compute_loss ): ",attention_mask.shape)

    # Labels: ignore prompt + input ,
    # shape: [1, 20 + 6 + 15] = [1, 41]
    # Model á€€á€­á€¯ training á€œá€¯á€•á€ºá€á€²á€·á€¡á€á€« prompt á€”á€²á€· input á€€á€­á€¯ loss á€™á€á€½á€€á€ºá€˜á€°á€¸áŠ target (output) á€€á€­á€¯á€•á€² loss á€á€½á€€á€ºá€™á€šá€ºá‹
    labels = torch.cat(
        [
            torch.full(
                (batch_size, N_PROMPT_TOKENS + input_ids.size(1)),
                -100,
                device=model.device,
                dtype=torch.long
            ),
            target_ids
        ],
        dim=1
    )
    print("labels shape  ( compute_loss ) : ",labels.shape)

    outputs = model(
        inputs_embeds=full_embeds,
        attention_mask=attention_mask,
        labels=labels
    )
    return outputs.loss



# ===============================
# 6. TRAIN PROMPT (WITH BETTER OUTPUT)
# ===============================
optimizer = torch.optim.AdamW(soft_prompt.parameters(), lr=LR)

print("\n" + "="*40)
print("ğŸš€ Starting Prompt Tuning Training Session")
print("="*40 + "\n")

for epoch in range(EPOCHS):
    total_loss = 0.0

    for inp, out in train_data:
        loss = compute_loss(inp, out)

        optimizer.zero_grad()
        loss.backward()

        # ğŸ”’ Prevent FP16 explosions
        torch.nn.utils.clip_grad_norm_(soft_prompt.parameters(), max_norm=1.0)

        optimizer.step()
        total_loss += loss.item()

    print(f"Epoch {epoch + 1:02d} | Loss: {total_loss:.4f}\n")

print("âœ¨ Training Complete!\n")


# ===============================
# 7. SAVE SOFT PROMPT
# ===============================
torch.save(soft_prompt.state_dict(), "soft_prompt_ros2.pt")
print("\nâœ… Soft prompt saved as soft_prompt_ros2.pt")

# ===============================
# 8. INFERENCE
# ===============================
def infer_ros2_command(human_input):
    input_ids = tokenizer(human_input, return_tensors="pt").input_ids.to(model.device)

    input_embeds = model.get_input_embeddings()(input_ids)
    # input_embeds shape: (1, input_seq_len, embedding_dim)

    # (1) á€†á€­á€¯á€á€¬ batch size á á€á€¯á€¡á€á€½á€€á€º soft prompt embeddings á€á€±á€¬á€„á€ºá€¸á€á€¬á€•á€«á‹
    # instance(batch_size) á€œá€­á€¯á€· á€á€±á€«á€ºá€œá€­á€¯á€€á€ºá€á€¬á€”á€²á€· forward(self, batch_size) á€€á€­á€¯ automatic á€á€±á€«á€ºá€•á€±á€¸á€•á€«á€á€šá€ºá‹
    # feedforward
    prompt_embeds = soft_prompt(1)
    print("\n,prompt_embeds shape ( infer_ros2_command ):", prompt_embeds.shape)

    full_embeds = torch.cat([prompt_embeds, input_embeds], dim=1)
    print("full_embeds shape ( infer_ros2_command ):", full_embeds.shape)


    with torch.no_grad():
        output_ids = model.generate(
            inputs_embeds=full_embeds,
            max_new_tokens=MAX_NEW_TOKENS,
            do_sample=False,
            pad_token_id=tokenizer.eos_token_id
        )

    return tokenizer.decode(output_ids[0], skip_special_tokens=True)

# ===============================
# 9. TEST
# ===============================
print("\nğŸ§ª Testing Prompt-Tuned Model\n")

tests = [
    "Move forward 9 meters",
    "Turn left 90 degrees",
    "Navigate to waypoint A"
]

for t in tests:
    print("Input :", t)
    print("\nOutput:", infer_ros2_command(t))
    print("-" * 80)